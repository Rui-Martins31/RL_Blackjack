\documentclass[12pt]{article}

\usepackage{geometry}

\geometry{a4paper,portrait,top=30mm,bottom=20mm,left=2.54cm,
	right=2.54cm,headsep=1mm,headheight=30mm,footskip=5mm}

\renewcommand{\familydefault}{\sfdefault}

\usepackage{sectsty}
\sectionfont{\fontsize{15}{18}\selectfont}
\subsectionfont{\fontsize{12}{15}\selectfont}

\usepackage[utf8]{inputenc} % accents i altres
\usepackage[T1]{fontenc}
\usepackage{xcolor}

\usepackage{multirow,tabularx}

\PassOptionsToPackage{hyphens}{url}
\usepackage[hidelinks]{hyperref}
\usepackage{graphicx}
\graphicspath{{Figures/}}

\newcommand{\course}{Dynamic Programming and Learning for Decision and Control}
\newcommand{\courseCode}{M.EEC026}
\newcommand{\lectiveYear}{2025--2026}
\newcommand{\semester}{1\textsuperscript{st}}
\newcommand{\degreeYear}{2\textsuperscript{nd}}

\usepackage{adjustbox}
\usepackage{lastpage}
%------------------------------ Header Footer ----------------------------------
\usepackage{fancyhdr}
	\pagestyle{fancyplain}					
	\fancyhead{}
	\fancyhead[]%
	{%
		\ifnum\thepage=1%
			\begin{minipage}{0.25\textwidth}
				{\includegraphics[width=0.9\textwidth]{images/logotipoUP.png}}
			\end{minipage}\hfill%
			\begin{minipage}{0.75\textwidth}
				\begin{flushleft}
					{\footnotesize\sc M.EEC $\vert$ \degreeYear{} Year}\\
					{\footnotesize\sc\courseCode{} $\vert$ \course{} $\vert$ \lectiveYear{} â€“- \semester{} Semester}
				\end{flushleft}
			\end{minipage}\\
			\rule[10pt]{\textwidth}{0.5pt}\\[-12pt]%
			\vspace*{1mm}
		\else
			\begin{adjustbox}{varwidth=\textwidth,raise=5mm}
				\centering
				\textbf{Intermediate Report}\\
				\rule[10pt]{\textwidth}{0.5pt}\\[-12pt]
			\end{adjustbox}
			\vspace*{-3mm}
		\fi%
	}
	\fancyfoot{}	
	\fancyfoot[CE,CO]%
		{%
			\footnotesize
			\rule{\textwidth}{0.5pt}\\[2mm]
			\thepage{}$/$\pageref{LastPage}%
		}
	\renewcommand{\headrulewidth}{0pt}					
	\renewcommand{\footrulewidth}{0pt}	
%-------------------------------------------------------------------------------

\usepackage{amstext,amsfonts,amsmath,amsthm,graphicx,amssymb,amscd,epsfig}
\usepackage{rotating}

\usepackage{subfig}
\captionsetup*[figure]{position=bottom}
\captionsetup*[subfigure]{position=bottom}


\usepackage{longtable}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{multirow}
\usepackage{float}
\usepackage{multicol}
\usepackage{wrapfig}

\usepackage[backend=biber, style=ieee]{biblatex}

% After page
\usepackage{afterpage}

% Listing - Python
\usepackage{listings}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codebackground}{rgb}{0.95,0.95,0.92}
\lstset{
    language=Python, 
    basicstyle=\footnotesize\ttfamily, % Font size and family
    backgroundcolor=\color{codebackground}, % Background color
    commentstyle=\color{codegray}, % Comments style
    keywordstyle=\color{blue}\bfseries, % Keywords style
    numberstyle=\tiny\color{codegray}, % Line number style
    stringstyle=\color{red}, % String style
    frame=tb, % Frame on top and bottom
    tabsize=4, % Tab space width
    showtabs=false, % Show tabs as symbols
    showspaces=false, % Show spaces as symbols
    showstringspaces=false, % Show spaces in strings
    breaklines=true, % Automatic line breaking
    captionpos=b, % Caption position at bottom
    floatplacement=H, % Place listing exactly where specified
}

\begin{document}

%/---------- Title ---------/
\begin{center}
	\Large
	\textbf{Blackjack Report}
\end{center}
	
%/---------- Author ---------/
\begin{center}
	\large
	Author: 202104636 Rui Filipe Castro Martins
\end{center}

%/---------- Main text body ---------/
\section{Introduction} \label{section:introduction}

    Reinforcement Learning (RL) is a powerful way to train models without relying specifically in supervised learning. Based on the concept of action-reward, the agent (the one to be trained) is placed inside the environment where it can interact with it according to its own observations of the environment. From those observations, the agent can act. Actions can be either benefitial (the agent gets closer to the goal) or detrimental (the agent can end up "losing").
    
    In this project, the main goal is to design and train an agent to successfully learn how to play a game of blackjack. However, it is worth noticing that this is not a traditional blackjack; this one is called Easy21 and is based on a set of rules that sets it apart from the original game played in casinos.

% ----
\section{Easy21} \label{section:easy21}

    Easy21 can be seen as a spinoff of the original Blackjack game, where, for instance, cards like aces and faces cannot be drawed and the probability of drawing a red card is smaller than the probability of drwaing a black one. Also, red card should be counted as negative values for the final sum while black card as positive.

    This simplified version allows the player to choose from two independent actions: HIT or STICK. The player, in a first moment, is showed two card: one corresponding to the initial card of the dealer, and one that belongs to himself. From this observation the player chooses wether he wants to gamble and receive another card by HITting or if he prefers to simply STICK to his current hand.

    The winner is found by summing the value of the cards in each hand and evaluating which one is greater or by checking if either the player or the dealer busted by going over 21 or below 1.

    There are four scenarios when it comes to the final result:
        \begin{itemize}
            \item Both player and dealer get the same value when summing their hands which means there is a DRAW;
            \item The player has a greater hand than the dealer and WINS;
            \item The dealer has a greater hand than the player and WINS;
            \item Either the player or the dealer busts and the other side WINS.
        \end{itemize}

    Given the nature of this game, it is necessary to create a custom environment class that handles all the logic behind it. The subsequent snippets of code, shown in Listing \ref{listing:class_easy21} highlight the most important parts of the codebase.
    
    This custom environment allows us to quickly generate synthetic data to train the agent. 

% ----
\section{Agent}

    To better implement a modular agent that adapts to different scenarios, a custom class and script were developed to guarantee this characteristic. Despite being based on past implementations such as the one found under OpenAI's Gymnasium documentation, this agent was fully built from scratch with simplicity in mind.

    As seen in Listing \ref{listing:class_agent}, a config file was created to quickly modify the parameters of both environment and agent without having to access them internally. Listing \ref{listing:config_file} briefly describes this config file.

    \subsection{Monte Carlo}
        
        A Monte Carlo simulation is a computer-based mathematical technique that uses repeated random sampling to model the probability of different outcomes in an uncertain process. Since, in this case, we developed a model of the environment and agent, it is possible to use Monte Carlo simulation to quickly come up with a steady state estimation of the Value Function.

        Some rules were stablished to ensure reproducibility and consistency. 
        \begin{itemize}
            \item Value function must be initialized to zero;
            \item The Learning Rate value must follow the equation \ref{equation:lr_vary};
                    \begin{equation} \label{equation:lr_vary}
                        \alpha_{t} = \frac{1}{N(s_{t},a_{t})}
                    \end{equation}
            \item The Epsilon value, the one who dictates the ration between exploration and exploitation, must follow the equation \ref{equation:epsilon_vary};
                    \begin{equation} \label{equation:epsilon_vary}
                        \epsilon_{t} = \frac{N_{0}}{N_{0} + N(s_{t})}
                    \end{equation}
        \end{itemize}

        By default, the expected value of N0 is 100, though other values should also be tested in case they produce better results in comparison.

        In the end, and with the output provided by the Monte Carlo simulation, it is possible to compute and plot a 3D visualization of the Value Function for different combinations of observations. A heatmap and a 3D representation of the Value Function were created and can be seen in Figures \ref{fig:monte_carlo_heat_map} and \ref{fig:monte_carlo_value_function}.

    \subsection{TD Learning}

        TD (Temporal Difference) learning is a type of reinforcement learning method that uses a combination of Monte Carlo and dynamic programming to predict the future value of a state.

        In our case we use the notion of SARSA to compute the desired metrics. SARSA (State-Action-Reward-State-Action) is an on-policy reinforcement learning algorithm that helps an agent to learn an optimal policy by interacting with its environment. The agent explores its environment, takes actions, receives feedback and continuously updates its behavior to maximize long-term rewards.

        The real difference between the Monte Carlo simulation method and SARSA comes down to the moment where each of one of them performs the update logic. Monte Carlo tracks the entire trajectory of the episode and only updates at the end of it while SARSA updates every time it selects an action and receives the instant reward.

        Additionally, the mean-squared error, as represented in Equation \ref{equation:mean_squared_error}, was computed every 1000 episodes to later compare the results for different $\lambda$ values.

        \begin{equation} \label{equation:mean_squared_error}
            \sum_{s,a} (Q(s, a) - Q^*(s, a))^2
        \end{equation}


% ----
\section{Results}

\input{results}
	

% ----
\clearpage
\section{Conclusions}

    This project successfully implemented and compared two fundamental reinforcement learning approaches for solving the Easy21 card game: Monte Carlo simulation and Temporal Difference (TD) learning with SARSA. The custom environment and modular agent architecture developed from scratch demonstrated the practical application of action-value methods in a stochastic decision-making scenario.

    The Monte Carlo approach, using first-visit updates with episode-based learning, converged to a stable value function after 1,000,000 episodes. The adaptive learning rate $\alpha_{t} = 1/N(s_{t},a_{t})$ and epsilon-greedy exploration strategy $\epsilon_{t} = N_{0}/(N_{0} + N(s_{t}))$ with $N_{0} = 100$ enabled effective balance between exploration and exploitation. The resulting policy heatmap and 3D value function visualization clearly illustrated the agent's learned decision boundaries for different combinations of player and dealer cards.

    The TD learning implementation with SARSA provided an alternative approach through step-by-step updates rather than episode-end batch updates. This online learning characteristic allowed the agent to adapt its policy during each episode, potentially offering faster convergence for certain state-action pairs. The mean-squared error metric computed every 1000 episodes provided quantitative assessment of learning progress and enabled comparison with the Monte Carlo baseline.

    Both methods successfully learned viable blackjack strategies, as evidenced by the value function visualizations presented in Figures \ref{fig:monte_carlo_heat_map}, \ref{fig:monte_carlo_value_function}, \ref{fig:td_learning_heat_map}, and \ref{fig:td_learning_value_function}. The implementation's modular design, with configuration-driven parameters and separation of concerns between environment, agent, and visualization components, facilitated systematic experimentation and reproducible results.

    The project demonstrated that reinforcement learning can effectively solve card game scenarios without explicit knowledge of optimal strategies, relying solely on trial-and-error interaction with the environment. The comparison between Monte Carlo and TD methods highlighted the trade-offs between episode-based and incremental learning approaches in the context of episodic tasks with terminal rewards.


% ----
\newpage
\section{Annexes}

\input{annexes}


% ----

\end{document}