\documentclass[12pt]{article}

\usepackage{geometry}

\geometry{a4paper,portrait,top=30mm,bottom=20mm,left=2.54cm,
	right=2.54cm,headsep=1mm,headheight=30mm,footskip=5mm}

\renewcommand{\familydefault}{\sfdefault}

\usepackage{sectsty}
\sectionfont{\fontsize{15}{18}\selectfont}
\subsectionfont{\fontsize{12}{15}\selectfont}

\usepackage[utf8]{inputenc} % accents i altres
\usepackage[T1]{fontenc}
\usepackage{xcolor}

\usepackage{multirow,tabularx}

\PassOptionsToPackage{hyphens}{url}
\usepackage[hidelinks]{hyperref}
\usepackage{graphicx}
\graphicspath{{Figures/}}

\newcommand{\course}{Dynamic Programming and Learning for Decision and Control}
\newcommand{\courseCode}{M.EEC026}
\newcommand{\lectiveYear}{2025--2026}
\newcommand{\semester}{1\textsuperscript{st}}
\newcommand{\degreeYear}{2\textsuperscript{nd}}

\usepackage{adjustbox}
\usepackage{lastpage}
%------------------------------ Header Footer ----------------------------------
\usepackage{fancyhdr}
	\pagestyle{fancyplain}					
	\fancyhead{}
	\fancyhead[]%
	{%
		\ifnum\thepage=1%
			\begin{minipage}{0.25\textwidth}
				{\includegraphics[width=0.9\textwidth]{images/logotipoUP.png}}
			\end{minipage}\hfill%
			\begin{minipage}{0.75\textwidth}
				\begin{flushleft}
					{\footnotesize\sc M.EEC $\vert$ \degreeYear{} Year}\\
					{\footnotesize\sc\courseCode{} $\vert$ \course{} $\vert$ \lectiveYear{} â€“- \semester{} Semester}
				\end{flushleft}
			\end{minipage}\\
			\rule[10pt]{\textwidth}{0.5pt}\\[-12pt]%
			\vspace*{1mm}
		\else
			\begin{adjustbox}{varwidth=\textwidth,raise=5mm}
				\centering
				\textbf{Intermediate Report}\\
				\rule[10pt]{\textwidth}{0.5pt}\\[-12pt]
			\end{adjustbox}
			\vspace*{-3mm}
		\fi%
	}
	\fancyfoot{}	
	\fancyfoot[CE,CO]%
		{%
			\footnotesize
			\rule{\textwidth}{0.5pt}\\[2mm]
			\thepage{}$/$\pageref{LastPage}%
		}
	\renewcommand{\headrulewidth}{0pt}					
	\renewcommand{\footrulewidth}{0pt}	
%-------------------------------------------------------------------------------

\usepackage{amstext,amsfonts,amsmath,amsthm,graphicx,amssymb,amscd,epsfig}
\usepackage{rotating}

\usepackage{subfig}
\captionsetup*[figure]{position=bottom}
\captionsetup*[subfigure]{position=bottom}


\usepackage{longtable}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{multirow}
\usepackage{float}
\usepackage{multicol}
\usepackage{wrapfig}

\usepackage[backend=biber, style=ieee]{biblatex}

% After page
\usepackage{afterpage}

% Listing - Python
\usepackage{listings}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codebackground}{rgb}{0.95,0.95,0.92}
\lstset{
    language=Python, 
    basicstyle=\footnotesize\ttfamily, % Font size and family
    backgroundcolor=\color{codebackground}, % Background color
    commentstyle=\color{codegray}, % Comments style
    keywordstyle=\color{blue}\bfseries, % Keywords style
    numberstyle=\tiny\color{codegray}, % Line number style
    stringstyle=\color{red}, % String style
    frame=tb, % Frame on top and bottom
    tabsize=4, % Tab space width
    showtabs=false, % Show tabs as symbols
    showspaces=false, % Show spaces as symbols
    showstringspaces=false, % Show spaces in strings
    breaklines=true, % Automatic line breaking
    captionpos=b, % Caption position at bottom
    floatplacement=H, % Place listing exactly where specified
}

\begin{document}

%/---------- Title ---------/
\begin{center}
	\Large
	\textbf{Blackjack Report}
\end{center}
	
%/---------- Author ---------/
\begin{center}
	\large
	Author: 202104636 Rui Filipe Castro Martins
\end{center}

%/---------- Main text body ---------/
\section{Introduction} \label{section:introduction}

    Reinforcement Learning (RL) is a powerful way to train models without relying specifically in supervised learning. Based on the concept of action-reward, the agent (the one to be trained) is placed inside the environment where it can interact with it according to its own observations of the environment. From those observations, the agent can act. Actions can be either benefitial (the agent gets closer to the goal) or detrimental (the agent can end up "losing").
    
    In this project, the main goal is to design and train an agent to successfully learn how to play a game of blackjack. However, it is worth noticing that this is not a traditional blackjack; this one is called Easy21 and is based on a set of rules that sets it apart from the original game played in casinos.

% ----
\section{Easy21} \label{section:easy21}

    Easy21 can be seen as a spinoff of the original Blackjack game, where, for instance, cards like aces and faces cannot be drawed and the probability of drawing a red card is smaller than the probability of drawing a black one. Also, red card should be counted as negative values for the final sum while black card as positive.

    This simplified version allows the player to choose from two independent actions: HIT or STICK. The player, in an initial moment, is showed two card: one corresponding to the initial card of the dealer, and one that belongs to himself. From this observation the player chooses wether he wants to gamble and receive another card by HITting or, if he prefers, to simply STICK to his current hand.

    The winner is found by summing the value of the cards in each hand and evaluating which one is greater or by checking if either the player or the dealer busted by going over 21 or below 1.

    There are four scenarios when it comes to the final result:
        \begin{itemize}
            \item Both player and dealer get the same value when summing their hands which means there is a DRAW;
            \item The player has a greater hand than the dealer and WINS;
            \item The dealer has a greater hand than the player and WINS;
            \item Either the player or the dealer busts and the other side WINS.
        \end{itemize}

    Given the nature of this game, it is necessary to create a custom environment class that handles all the logic behind it. The subsequent snippets of code, shown in Listing \ref{listing:class_easy21}, highlight the most important parts of the codebase.
    
    This custom environment allows us to quickly generate synthetic data to train the agent. 

% ----
\section{Agent}

    To better implement a modular agent that adapts to different scenarios, a custom class and script were developed to guarantee this characteristic. Despite being based on past implementations such as the one found under OpenAI's Gymnasium documentation, this agent was fully built from scratch with simplicity in mind.

    As seen in Listing \ref{listing:class_agent}, a config file was created to quickly modify the parameters of both environment and agent without having to access them internally. Listing \ref{listing:config_file} briefly describes this config file.

    \subsection{Monte Carlo}
        
        A Monte Carlo simulation is a computer-based mathematical technique that uses repeated random sampling to model the probability of different outcomes in an uncertain process. Since, in this case, we developed a model of the environment and agent, it is possible to use Monte Carlo simulation to quickly come up with a steady state estimation of the Value Function.

        Some rules were stablished to ensure reproducibility and consistency. 
        \begin{itemize}
            \item Value function must be initialized to zero;
            \item The Learning Rate value must follow the equation \ref{equation:lr_vary};
                    \begin{equation} \label{equation:lr_vary}
                        \alpha_{t} = \frac{1}{N(s_{t},a_{t})}
                    \end{equation}
            \item The Epsilon value, the one who dictates the ration between exploration and exploitation, must follow the equation \ref{equation:epsilon_vary};
                    \begin{equation} \label{equation:epsilon_vary}
                        \epsilon_{t} = \frac{N_{0}}{N_{0} + N(s_{t})}
                    \end{equation}
        \end{itemize}

        By default, the expected value of $N_{0}$ is 100, though other values should also be tested in case they produce better results in comparison.

        In the end, and with the output provided by the Monte Carlo simulation, it is possible to compute and plot a 3D visualization of the Value Function for different combinations of observations. A heatmap and a 3D representation of the Value Function were created and can be seen in Figures \ref{fig:monte_carlo_heat_map} and \ref{fig:monte_carlo_value_function}.

    \subsection{TD Learning with Sarsa($\lambda$)}

        TD (Temporal Difference) learning is a reinforcement learning method that combines ideas from Monte Carlo and dynamic programming to predict the value of states. Unlike Monte Carlo methods that wait until the end of an episode, TD learning updates value estimates after each step based on subsequent estimates.

        In this project, Sarsa($\lambda$) was implemented, which extends the basic Sarsa algorithm with eligibility traces. Sarsa (State-Action-Reward-State-Action) is an on-policy reinforcement learning algorithm that learns by following its current policy. The addition of the $\lambda$ parameter allows the algorithm to bridge between one-step TD learning ($\lambda = 0$) and Monte Carlo methods ($\lambda = 1$).

        \subsubsection{Eligibility Traces}

        Eligibility traces provide a mechanism for efficient credit assignment by maintaining a memory of which state-action pairs have been recently visited. The trace for each state-action pair is updated as shown in Equation \ref{equation:eligibility_trace}.

        \begin{equation} \label{equation:eligibility_trace}
            E(s_t, a_t) \leftarrow E(s_t, a_t) + 1
        \end{equation}

        After each step, all eligibility traces decay according to Equation \ref{equation:trace_decay}.

        \begin{equation} \label{equation:trace_decay}
            E(s, a) \leftarrow \gamma \lambda E(s, a)
        \end{equation}

        where $\gamma$ is the discount factor (set to 1.0 for this undiscounted task) and $\lambda$ controls the degree of bootstrapping.

        \subsubsection{Update Rule}

        The Q-value updates in Sarsa($\lambda$) use the TD error $\delta_t$ and eligibility traces as shown in Equations \ref{equation:td_error} and \ref{equation:sarsa_lambda_update}.

        \begin{equation} \label{equation:td_error}
            \delta_t = r_t + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)
        \end{equation}

        \begin{equation} \label{equation:sarsa_lambda_update}
            Q(s, a) \leftarrow Q(s, a) + \alpha_t \delta_t E(s, a), \quad \forall s, a
        \end{equation}

        This update rule allows the TD error to propagate backward through the episode to all previously visited state-action pairs, weighted by their eligibility traces.

        \subsubsection{Experimental Setup}

        The same learning rate and exploration schedules from the Monte Carlo implementation were maintained for consistency:
        \begin{itemize}
            \item Learning rate: $\alpha_{t} = 1/N(s_{t},a_{t})$
            \item Exploration rate: $\epsilon_{t} = N_{0}/(N_{0} + N(s_{t}))$ with $N_{0} = 100$
        \end{itemize}

        Experiments were conducted for $\lambda \in \{0.0, 0.1, 0.2, \ldots, 1.0\}$, with each configuration running for 1000 episodes. The mean-squared error (MSE), defined in Equation \ref{equation:mean_squared_error}, was computed to compare the learned Q-values against the optimal values $Q^*$ obtained from the Monte Carlo simulation with 1,000,000 episodes.

        \begin{equation} \label{equation:mean_squared_error}
            \text{MSE} = \sum_{s,a} (Q(s, a) - Q^*(s, a))^2
        \end{equation}

        For $\lambda = 0$ and $\lambda = 1$, the MSE was tracked at each episode to analyze the learning dynamics throughout training.


% ----
\section{Results}

\input{results}
	

% ----
\clearpage
\section{Conclusions}

    This project successfully implemented and compared two fundamental reinforcement learning approaches for solving the Easy21 card game: Monte Carlo simulation and Temporal Difference learning with Sarsa($\lambda$). The custom environment and modular agent architecture developed from scratch demonstrated the practical application of action-value methods in a stochastic scenario.

    The Monte Carlo approach, using first-visit updates with episode-based learning, converged to a stable value function after 1,000,000 episodes. The adaptive learning rate and epsilon-greedy exploration strategy with enabled effective balance between exploration and exploitation. The resulting policy heatmap and 3D value function visualization clearly illustrated the agent's learned decision boundaries for different combinations of player and dealer cards. This optimal Q* matrix served as the ground truth for evaluating the TD learning experiments.

    The Sarsa($\lambda$) implementation with eligibility traces provided a comprehensive study of the bias-variance tradeoff in TD learning. Experiments across 11 different $\lambda$ values revealed that $\lambda = 0.1$ achieved the lowest MSE (0.204343) after 1000 episodes, outperforming both pure one-step Sarsa ($\lambda = 0$, MSE = 0.237316) and Monte Carlo-like methods ($\lambda = 1$, MSE = 0.482572). This demonstrates that a small amount of eligibility trace provides the optimal balance between immediate TD updates and full episode backups when training with limited samples.

    The learning curve analysis comparing $\lambda = 0$ and $\lambda = 1$ highlighted a fundamental insight: TD methods converge faster with fewer episodes by bootstrapping from current estimates, while Monte Carlo methods require substantially more samples to achieve low-variance Q-value estimates. The progressive degradation of performance as $\lambda$ increases beyond 0.1 indicates that excessive credit assignment to distant state-action pairs introduces more noise than signal in the early stages of learning.

    The modular design proposed in this project and separation of concerns between environment, agent and visualization components facilitated experimentation.

    In conclusion, the comparison between Monte Carlo and Sarsa($\lambda$) methods highlighted important trade-offs: Monte Carlo provides unbiased estimates but requires many episodes to converge while TD methods with appropriate $\lambda$ values offer faster learning but may introduce bias. For this particular task, Sarsa with small eligibility traces ($\lambda = 0.1$) represents the most sample-efficient approach.


% ----
\newpage
\section{Annexes}

\input{annexes}


% ----

\end{document}