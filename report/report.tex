\documentclass[12pt]{article}

\usepackage{geometry}

\geometry{a4paper,portrait,top=30mm,bottom=20mm,left=2.54cm,
	right=2.54cm,headsep=1mm,headheight=30mm,footskip=5mm}

\renewcommand{\familydefault}{\sfdefault}

\usepackage{sectsty}
\sectionfont{\fontsize{15}{18}\selectfont}
\subsectionfont{\fontsize{12}{15}\selectfont}

\usepackage[utf8]{inputenc} % accents i altres
\usepackage[T1]{fontenc}
\usepackage{xcolor}

\usepackage{multirow,tabularx}

\PassOptionsToPackage{hyphens}{url}
\usepackage[hidelinks]{hyperref}
\usepackage{graphicx}
\graphicspath{{Figures/}}

\newcommand{\course}{Dynamic Programming and Learning for Decision and Control}
\newcommand{\courseCode}{M.EEC026}
\newcommand{\lectiveYear}{2025--2026}
\newcommand{\semester}{1\textsuperscript{st}}
\newcommand{\degreeYear}{2\textsuperscript{nd}}

\usepackage{adjustbox}
\usepackage{lastpage}
%------------------------------ Header Footer ----------------------------------
\usepackage{fancyhdr}
	\pagestyle{fancyplain}					
	\fancyhead{}
	\fancyhead[]%
	{%
		\ifnum\thepage=1%
			\begin{minipage}{0.25\textwidth}
				{\includegraphics[width=0.9\textwidth]{images/logotipoUP.png}}
			\end{minipage}\hfill%
			\begin{minipage}{0.75\textwidth}
				\begin{flushleft}
					{\footnotesize\sc M.EEC $\vert$ \degreeYear{} Year}\\
					{\footnotesize\sc\courseCode{} $\vert$ \course{} $\vert$ \lectiveYear{} â€“- \semester{} Semester}
				\end{flushleft}
			\end{minipage}\\
			\rule[10pt]{\textwidth}{0.5pt}\\[-12pt]%
			\vspace*{1mm}
		\else
			\begin{adjustbox}{varwidth=\textwidth,raise=5mm}
				\centering
				\textbf{Intermediate Report}\\
				\rule[10pt]{\textwidth}{0.5pt}\\[-12pt]
			\end{adjustbox}
			\vspace*{-3mm}
		\fi%
	}
	\fancyfoot{}	
	\fancyfoot[CE,CO]%
		{%
			\footnotesize
			\rule{\textwidth}{0.5pt}\\[2mm]
			\thepage{}$/$\pageref{LastPage}%
		}
	\renewcommand{\headrulewidth}{0pt}					
	\renewcommand{\footrulewidth}{0pt}	
%-------------------------------------------------------------------------------

\usepackage{amstext,amsfonts,amsmath,amsthm,graphicx,amssymb,amscd,epsfig}
\usepackage{rotating}

\usepackage{subfig}
\captionsetup*[figure]{position=bottom}
\captionsetup*[subfigure]{position=bottom}


\usepackage{longtable}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{multirow}
\usepackage{float}
\usepackage{multicol}
\usepackage{wrapfig}

\usepackage[backend=biber, style=ieee]{biblatex}

% After page
\usepackage{afterpage}

% Listing - Python
\usepackage{listings}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codebackground}{rgb}{0.95,0.95,0.92}
\lstset{
    language=Python, 
    basicstyle=\footnotesize\ttfamily, % Font size and family
    backgroundcolor=\color{codebackground}, % Background color
    commentstyle=\color{codegray}, % Comments style
    keywordstyle=\color{blue}\bfseries, % Keywords style
    numberstyle=\tiny\color{codegray}, % Line number style
    stringstyle=\color{red}, % String style
    frame=tb, % Frame on top and bottom
    tabsize=4, % Tab space width
    showtabs=false, % Show tabs as symbols
    showspaces=false, % Show spaces as symbols
    showstringspaces=false, % Show spaces in strings
    breaklines=true, % Automatic line breaking
    captionpos=b, % Caption position at bottom
    floatplacement=H, % Place listing exactly where specified
}

\begin{document}

%/---------- Title ---------/
\begin{center}
	\Large
	\textbf{Blackjack Report}
\end{center}
	
%/---------- Author ---------/
\begin{center}
	\large
	Author: 202104636 Rui Filipe Castro Martins
\end{center}

%/---------- Main text body ---------/
\section{Introduction} \label{section:introduction}

    Reinforcement Learning (RL) is a powerful way to train models without relying specifically in supervised learning. Based on the concept of action-reward, the agent (the one to be trained) is placed inside the environment where it can interact with it according to its own observations of the environment. From those observations, the agent can act. Actions can be either benefitial (the agent gets closer to the goal) or detrimental (the agent can end up "losing").
    
    In this project, the main goal is to design and train an agent to successfully learn how to play a game of blackjack. However, it is worth noticing that this is not a traditional blackjack; this one is called Easy21 and is based on a set of rules that sets it apart from the original game played in casinos.

% ----
\section{Easy21} \label{section:easy21}

    Easy21 can be seen as a spinoff of the original Blackjack game, where, for instance, cards like aces and faces cannot be drawed and the probability of drawing a red card is smaller than the probability of drwaing a black one. Also, red card should be counted as negative values for the final sum while black card as positive.

    This simplified version allows the player to choose from two independent actions: HIT or STICK. The player, in a first moment, is showed two card: one corresponding to the initial card of the dealer, and one that belongs to himself. From this observation the player chooses wether he wants to gamble and receive another card by HITting or if he prefers to simply STICK to his current hand.

    The winner is found by summing the value of the cards in each hand and evaluating which one is greater or by checking if either the player or the dealer busted by going over 21 or below 1.

    There are four scenarios when it comes to the final result:
        \begin{itemize}
            \item Both player and dealer get the same value when summing their hands which means there is a DRAW;
            \item The player has a greater hand than the dealer and WINS;
            \item The dealer has a greater hand than the player and WINS;
            \item Either the player or the dealer busts and the other side WINS.
        \end{itemize}

    Given the nature of this game, it is necessary to create a custom environment class that handles all the logic behind it. The subsequent snippets of code, shown in Listing \ref{listing:class_easy21} highlight the most important parts of the codebase.
    
    This custom environment allows us to quickly generate synthetic data to train the agent. 

% ----
\section{Agent}

    To better implement a modular agent that adapts to different scenarios, a custom class and script were developed to guarantee this characteristic. Despite being based on past implementations such as the one found under OpenAI's Gymnasium documentation, this agent was fully built from scratch with simplicity in mind.

    As seen in Listing \ref{listing:class_agent}, a config file was created to quickly modify the parameters of both environment and agent without having to access them internally. Listing \ref{listing:config_file} briefly describes this config file.

    \subsection{Monte Carlo}
        
        A Monte Carlo simulation is a computer-based mathematical technique that uses repeated random sampling to model the probability of different outcomes in an uncertain process. Since, in this case, we developed a model of the environment and agent, it is possible to use Monte Carlo simulation to quickly come up with a steady state estimation of the Value Function.

        Some rules were stablished to ensure reproducibility and consistency. 
        \begin{itemize}
            \item Value function must be initialized to zero;
            \item The Learning Rate value must follow the equation \ref{equation:lr_vary};
                    \begin{equation} \label{equation:lr_vary}
                        \alpha_{t} = \frac{1}{N(s_{t},a_{t})}
                    \end{equation}
            \item The Epsilon value, the one who dictates the ration between exploration and exploitation, must follow the equation \ref{equation:epsilon_vary};
                    \begin{equation} \label{equation:epsilon_vary}
                        \epsilon_{t} = \frac{N_{0}}{N_{0} + N(s_{t})}
                    \end{equation}
        \end{itemize}

        By default, the expected value of N0 is 100, though other values should also be tested in case they produce better results in comparison.

        In the end, and with the output provided by the Monte Carlo simulation, it is possible to compute and plot a 3D visualization of the Value Function for different combinations of observations. A heatmap and a 3D representation of the Value Function were created and can be seen in Figures \ref{fig:heat_map} and \ref{fig:value_function}.

    \subsection{TD Learning}

        SARSA (State-Action-Reward-State-Action) is an on-policy reinforcement learning algorithm that helps an agent to learn an optimal policy by interacting with its environment. The agent explores its environment, takes actions, receives feedback and continuously updates its behavior to maximize long-term rewards.


% ----
\section{Results}

    Heat Map:

        \begin{figure}[h]
        	\centering
        	\includegraphics[width=0.75\linewidth]{../model/images/policy_heatmap.png}
        	\caption{Heat Map.}
        	\label{fig:heat_map}
        \end{figure}


    3D Map of the Value Function:

        \begin{figure}[h]
        	\centering
        	\includegraphics[width=0.75\linewidth]{../model/images/value_function_3d.png}
        	\caption{Value function represented using a 3D chart.}
        	\label{fig:value_function}
        \end{figure}
	

% ----
\newpage
\section{Conclusions}

	
% ----
\newpage
\section{Annexes}

\include{annexes}


% ----

\end{document}